{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the sliding window attention receptive field\n",
    "\n",
    "The receptive field comes from convolutional neural networks. It is the region in the input space that a particular CNN's feature is looking at. The receptive field is a function of the CNN's architecture and the number of layers and is a useful concept for understanding the CNN's feature extraction process.\n",
    "\n",
    "This sliding window attention, we start with our input sequence, in which each token is represented by a vector. We then compute the attention using a sliding window of size $w$. The output of the self-attention (computed by the final multiplication of the attention weights and the input sequence) is a weighted sum of the input sequence. In our case, I will represent the input as a sequence of `set` objects, such that each set represents the tokens from which information has been captured by the attention mechanism.\n",
    "\n",
    "Initially, the sequence is a list of sets, all containing a single token.\n",
    "\n",
    "```\n",
    "Layer 1 input:\n",
    "0: ['the']\n",
    "1: ['cat']\n",
    "2: ['is']\n",
    "3: ['on']\n",
    "4: ['a']\n",
    "5: ['chair']\n",
    "```\n",
    "\n",
    "After the first layer, considering a sliding window size of 3, the output of the attention mechanism is:\n",
    "\n",
    "```\n",
    "Layer 1 output:\n",
    "0: ['the']\n",
    "1: ['the', 'cat']\n",
    "2: ['the', 'cat', 'is']\n",
    "3: ['cat', 'is', 'on']\n",
    "4: ['is', 'on', 'a']\n",
    "5: ['on', 'a', 'chair']\n",
    "```\n",
    "\n",
    "The output of the first layer becomes the input of the second layer. The output of the second layer is:\n",
    "\n",
    "```\n",
    "Layer 2 output:\n",
    "0: ['the']\n",
    "1: ['the', 'cat']\n",
    "2: ['the', 'cat', 'is']\n",
    "3: ['the', 'cat', 'is', 'on']\n",
    "4: ['the', 'cat', 'is', 'on', 'a']\n",
    "5: ['cat', 'is', 'on', 'a', 'chair']\n",
    "```\n",
    "\n",
    "As we can see, even with a sliding window of size 3, after just two layers, the attention mechanism can capture long-range dependencies. This is because the output of the first layer is used as the input of the second layer, and the attention mechanism is applied again. This is similar to the idea of stacking multiple layers of CNNs to increase the receptive field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'the'}, {'cat'}, {'is'}, {'on'}, {'a'}, {'chair'}]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of tuple with the token and the time-step\n",
    "#print_order = [chr(i) for i in range(ord('A'), ord('Z') + 1)]\n",
    "print_order = ['the', 'cat', 'is', 'on', 'a', 'chair']\n",
    "sequence = [{print_order[i]} for i in range(len(print_order))]\n",
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_attention(seq: list[set[str]], w: int):\n",
    "    seq_len = len(seq)\n",
    "    attention_scores: list[list[set]] = [[None for _ in range(seq_len)] for _ in range(seq_len)]\n",
    "    for i, q_tokens_set in enumerate(seq):\n",
    "        for j, k_tokens_set in enumerate(seq):\n",
    "            # The upper triangle is all None\n",
    "            if j > i:\n",
    "                continue\n",
    "            # Each token can only attend to the previous W tokens\n",
    "            if i - j >= w:\n",
    "                continue\n",
    "\n",
    "            attention = set()\n",
    "            # Add all tokens from q_tokens_set to attention_result\n",
    "            attention.update(q_tokens_set)\n",
    "            # Add all tokens from k_tokens_set to attention_resul\n",
    "            attention.update(k_tokens_set)\n",
    "\n",
    "            attention_scores[i][j] = attention\n",
    "    return attention_scores\n",
    "\n",
    "def multiple_by_v(attention_scores: list[list[set]], v_sequence: list[set[str]]) -> list[set[str]]:\n",
    "    seq_len = len(v_sequence)\n",
    "    result = [set() for _ in range(seq_len)]\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            attention = attention_scores[i][j]\n",
    "            v = v_sequence[j]\n",
    "            r = result[i]\n",
    "            # Add all the tokens in the attention (if not None) to r\n",
    "            if attention is not None:\n",
    "                # Add all the tokens in v to r\n",
    "                r.update(v)\n",
    "                r.update(attention)\n",
    "    return result\n",
    "\n",
    "def print_attention(attention_scores: list[list[set[str]]]):\n",
    "    for i, row in enumerate(attention_scores):\n",
    "        for j, attention in enumerate(row):\n",
    "            if attention is None:\n",
    "                print('None', end='\\t')\n",
    "            else:\n",
    "                print(f'{sorted(attention, key=lambda x: print_order.index(x))}', end='\\t')\n",
    "        print()\n",
    "\n",
    "def print_sequence(seq: list[set[str]]):\n",
    "    for i, tokens_set in enumerate(seq):\n",
    "        print(f'{i}: {sorted(tokens_set, key=lambda x: print_order.index(x))}')\n",
    "\n",
    "def print_layer(input: list[set[str]], layer_num: int) -> list[set[str]]:\n",
    "    print(f'Layer {layer_num} input:')\n",
    "    print_sequence(input)\n",
    "    attention_scores = sliding_window_attention(input, 3)\n",
    "    print(f'Layer {layer_num} attention scores:')\n",
    "    print_attention(attention_scores)\n",
    "    output = multiple_by_v(attention_scores, input)\n",
    "    print(f'Layer {layer_num} output:')\n",
    "    print_sequence(output)\n",
    "    print()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 input:\n",
      "0: ['the']\n",
      "1: ['cat']\n",
      "2: ['is']\n",
      "3: ['on']\n",
      "4: ['a']\n",
      "5: ['chair']\n",
      "Layer 1 attention scores:\n",
      "['the']\tNone\tNone\tNone\tNone\tNone\t\n",
      "['the', 'cat']\t['cat']\tNone\tNone\tNone\tNone\t\n",
      "['the', 'is']\t['cat', 'is']\t['is']\tNone\tNone\tNone\t\n",
      "None\t['cat', 'on']\t['is', 'on']\t['on']\tNone\tNone\t\n",
      "None\tNone\t['is', 'a']\t['on', 'a']\t['a']\tNone\t\n",
      "None\tNone\tNone\t['on', 'chair']\t['a', 'chair']\t['chair']\t\n",
      "Layer 1 output:\n",
      "0: ['the']\n",
      "1: ['the', 'cat']\n",
      "2: ['the', 'cat', 'is']\n",
      "3: ['cat', 'is', 'on']\n",
      "4: ['is', 'on', 'a']\n",
      "5: ['on', 'a', 'chair']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Layer 1\n",
    "output_layer_1 = print_layer(sequence, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 2 input:\n",
      "0: ['the']\n",
      "1: ['the', 'cat']\n",
      "2: ['the', 'cat', 'is']\n",
      "3: ['cat', 'is', 'on']\n",
      "4: ['is', 'on', 'a']\n",
      "5: ['on', 'a', 'chair']\n",
      "Layer 2 attention scores:\n",
      "['the']\tNone\tNone\tNone\tNone\tNone\t\n",
      "['the', 'cat']\t['the', 'cat']\tNone\tNone\tNone\tNone\t\n",
      "['the', 'cat', 'is']\t['the', 'cat', 'is']\t['the', 'cat', 'is']\tNone\tNone\tNone\t\n",
      "None\t['the', 'cat', 'is', 'on']\t['the', 'cat', 'is', 'on']\t['cat', 'is', 'on']\tNone\tNone\t\n",
      "None\tNone\t['the', 'cat', 'is', 'on', 'a']\t['cat', 'is', 'on', 'a']\t['is', 'on', 'a']\tNone\t\n",
      "None\tNone\tNone\t['cat', 'is', 'on', 'a', 'chair']\t['is', 'on', 'a', 'chair']\t['on', 'a', 'chair']\t\n",
      "Layer 2 output:\n",
      "0: ['the']\n",
      "1: ['the', 'cat']\n",
      "2: ['the', 'cat', 'is']\n",
      "3: ['the', 'cat', 'is', 'on']\n",
      "4: ['the', 'cat', 'is', 'on', 'a']\n",
      "5: ['cat', 'is', 'on', 'a', 'chair']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Layer 2\n",
    "output_layer_2 = print_layer(output_layer_1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 3 input:\n",
      "0: ['the']\n",
      "1: ['the', 'cat']\n",
      "2: ['the', 'cat', 'is']\n",
      "3: ['the', 'cat', 'is', 'on']\n",
      "4: ['the', 'cat', 'is', 'on', 'a']\n",
      "5: ['cat', 'is', 'on', 'a', 'chair']\n",
      "Layer 3 attention scores:\n",
      "['the']\tNone\tNone\tNone\tNone\tNone\t\n",
      "['the', 'cat']\t['the', 'cat']\tNone\tNone\tNone\tNone\t\n",
      "['the', 'cat', 'is']\t['the', 'cat', 'is']\t['the', 'cat', 'is']\tNone\tNone\tNone\t\n",
      "None\t['the', 'cat', 'is', 'on']\t['the', 'cat', 'is', 'on']\t['the', 'cat', 'is', 'on']\tNone\tNone\t\n",
      "None\tNone\t['the', 'cat', 'is', 'on', 'a']\t['the', 'cat', 'is', 'on', 'a']\t['the', 'cat', 'is', 'on', 'a']\tNone\t\n",
      "None\tNone\tNone\t['the', 'cat', 'is', 'on', 'a', 'chair']\t['the', 'cat', 'is', 'on', 'a', 'chair']\t['cat', 'is', 'on', 'a', 'chair']\t\n",
      "Layer 3 output:\n",
      "0: ['the']\n",
      "1: ['the', 'cat']\n",
      "2: ['the', 'cat', 'is']\n",
      "3: ['the', 'cat', 'is', 'on']\n",
      "4: ['the', 'cat', 'is', 'on', 'a']\n",
      "5: ['the', 'cat', 'is', 'on', 'a', 'chair']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Layer 3\n",
    "output_layer_3 = print_layer(output_layer_2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 4 input:\n",
      "0: ['the']\n",
      "1: ['the', 'cat']\n",
      "2: ['the', 'cat', 'is']\n",
      "3: ['the', 'cat', 'is', 'on']\n",
      "4: ['the', 'cat', 'is', 'on', 'a']\n",
      "5: ['the', 'cat', 'is', 'on', 'a', 'chair']\n",
      "Layer 4 attention scores:\n",
      "['the']\tNone\tNone\tNone\tNone\tNone\t\n",
      "['the', 'cat']\t['the', 'cat']\tNone\tNone\tNone\tNone\t\n",
      "['the', 'cat', 'is']\t['the', 'cat', 'is']\t['the', 'cat', 'is']\tNone\tNone\tNone\t\n",
      "None\t['the', 'cat', 'is', 'on']\t['the', 'cat', 'is', 'on']\t['the', 'cat', 'is', 'on']\tNone\tNone\t\n",
      "None\tNone\t['the', 'cat', 'is', 'on', 'a']\t['the', 'cat', 'is', 'on', 'a']\t['the', 'cat', 'is', 'on', 'a']\tNone\t\n",
      "None\tNone\tNone\t['the', 'cat', 'is', 'on', 'a', 'chair']\t['the', 'cat', 'is', 'on', 'a', 'chair']\t['the', 'cat', 'is', 'on', 'a', 'chair']\t\n",
      "Layer 4 output:\n",
      "0: ['the']\n",
      "1: ['the', 'cat']\n",
      "2: ['the', 'cat', 'is']\n",
      "3: ['the', 'cat', 'is', 'on']\n",
      "4: ['the', 'cat', 'is', 'on', 'a']\n",
      "5: ['the', 'cat', 'is', 'on', 'a', 'chair']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Layer 4\n",
    "output_layer_4 = print_layer(output_layer_3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 5 input:\n",
      "0: ['the']\n",
      "1: ['the', 'cat']\n",
      "2: ['the', 'cat', 'is']\n",
      "3: ['the', 'cat', 'is', 'on']\n",
      "4: ['the', 'cat', 'is', 'on', 'a']\n",
      "5: ['the', 'cat', 'is', 'on', 'a', 'chair']\n",
      "Layer 5 attention scores:\n",
      "['the']\tNone\tNone\tNone\tNone\tNone\t\n",
      "['the', 'cat']\t['the', 'cat']\tNone\tNone\tNone\tNone\t\n",
      "['the', 'cat', 'is']\t['the', 'cat', 'is']\t['the', 'cat', 'is']\tNone\tNone\tNone\t\n",
      "None\t['the', 'cat', 'is', 'on']\t['the', 'cat', 'is', 'on']\t['the', 'cat', 'is', 'on']\tNone\tNone\t\n",
      "None\tNone\t['the', 'cat', 'is', 'on', 'a']\t['the', 'cat', 'is', 'on', 'a']\t['the', 'cat', 'is', 'on', 'a']\tNone\t\n",
      "None\tNone\tNone\t['the', 'cat', 'is', 'on', 'a', 'chair']\t['the', 'cat', 'is', 'on', 'a', 'chair']\t['the', 'cat', 'is', 'on', 'a', 'chair']\t\n",
      "Layer 5 output:\n",
      "0: ['the']\n",
      "1: ['the', 'cat']\n",
      "2: ['the', 'cat', 'is']\n",
      "3: ['the', 'cat', 'is', 'on']\n",
      "4: ['the', 'cat', 'is', 'on', 'a']\n",
      "5: ['the', 'cat', 'is', 'on', 'a', 'chair']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Layer 5\n",
    "output_layer_5 = print_layer(output_layer_4, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-mistral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
